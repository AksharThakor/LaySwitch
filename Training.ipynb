{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19792102",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os, re, json, warnings\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, callbacks\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.model_selection import LeaveOneGroupOut\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.style.use(\"seaborn-v0_8-darkgrid\")\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "DATA_DIR = \"./Data\"     \n",
    "OUTPUT_DIR = \"./Outputs2\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "WINDOW_SIZE = 120          # 2s @ 60Hz\n",
    "STRIDE = 1                 # stride-1: 1-120, 2-121, ...\n",
    "EPOCHS = 30\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "REQUIRE_CONSTANT_LABEL = True  #drop any window that spans label changes\n",
    "USE_NORMALIZATION = True       #True= fixed input z-norm baked into model\n",
    "\n",
    "FEATURE_COLS = [\"ax\", \"ay\", \"az\", \"gx\", \"gy\", \"gz\"]\n",
    "LABEL_COL = \"handedness\"\n",
    "TIME_COL = \"timestamp\"  \n",
    "\n",
    "\n",
    "ACCEPTED_LABELS = {\"left\": \"LEFT\", \"right\": \"RIGHT\", \"both\": \"BOTH\"}\n",
    "\n",
    "def _standardize_cols(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    return df.rename(columns={c: c.strip().lower() for c in df.columns})\n",
    "\n",
    "def _label_from_filename(stem: str):\n",
    "    s = stem.lower()\n",
    "    for tok in [\"left\", \"right\", \"both\"]:\n",
    "        if re.search(rf\"(^|[_\\-\\s]){tok}([_\\-\\s]|$)\", s):\n",
    "            return ACCEPTED_LABELS[tok]\n",
    "    return None\n",
    "\n",
    "def load_imu_csv(path: Path) -> pd.DataFrame:\n",
    "    df = pd.read_csv(path)\n",
    "    df = _standardize_cols(df)\n",
    "    required = set([TIME_COL, LABEL_COL, *FEATURE_COLS])\n",
    "    missing = required - set(df.columns)\n",
    "    if missing:\n",
    "        raise ValueError(f\"{path.name}: missing columns: {sorted(list(missing))}\")\n",
    "   \n",
    "    df[LABEL_COL] = (df[LABEL_COL].astype(str)\n",
    "                     .str.strip().str.lower()\n",
    "                     .map(ACCEPTED_LABELS).fillna(df[LABEL_COL]))\n",
    "    \n",
    "    inferred = _label_from_filename(path.stem)\n",
    "    if inferred is not None:\n",
    "        uniq = df[LABEL_COL].unique()\n",
    "        if len(uniq) == 1 and uniq[0] != inferred:\n",
    "            print(f\"[WARN] Label mismatch {path.name}: in-file={uniq[0]} vs name={inferred}\")\n",
    "    return df\n",
    "\n",
    "def make_stride_windows(df: pd.DataFrame,\n",
    "                        file_id: str,\n",
    "                        window_size: int,\n",
    "                        stride: int,\n",
    "                        require_constant_label: bool = True):\n",
    "    \n",
    "    n = len(df)\n",
    "    if n < window_size:\n",
    "        return np.empty((0, window_size, len(FEATURE_COLS)), dtype=np.float32), np.array([]), []\n",
    "\n",
    "    feats = df[FEATURE_COLS].to_numpy(dtype=np.float32)\n",
    "    labels = df[LABEL_COL].to_numpy()\n",
    "\n",
    "    X_list, y_list, g_list = [], [], []\n",
    "    for start in range(0, n - window_size + 1, stride):\n",
    "        end = start + window_size\n",
    "        y_win = labels[start:end]\n",
    "        if require_constant_label:\n",
    "            if not (y_win == y_win[0]).all():\n",
    "                continue\n",
    "            y_label = y_win[0]\n",
    "        else:\n",
    "            vals, counts = np.unique(y_win, return_counts=True)\n",
    "            y_label = vals[np.argmax(counts)]\n",
    "        X_list.append(feats[start:end])\n",
    "        y_list.append(y_label)\n",
    "        g_list.append(file_id)\n",
    "\n",
    "    if not X_list:\n",
    "        return np.empty((0, window_size, len(FEATURE_COLS)), dtype=np.float32), np.array([]), []\n",
    "    return np.stack(X_list, axis=0), np.array(y_list), g_list\n",
    "\n",
    "def load_and_window_all(data_dir: str, window_size: int, stride: int,\n",
    "                        require_constant_label: bool = True):\n",
    "    all_X, all_y, groups = [], [], []\n",
    "    files = sorted([f for f in Path(data_dir).glob(\"*.csv\")])\n",
    "    if not files:\n",
    "        raise FileNotFoundError(f\"No CSV files found in {data_dir}\")\n",
    "\n",
    "    for f in tqdm(files, desc=\"Loading CSVs\"):\n",
    "        df = load_imu_csv(f)\n",
    "        Xf, yf, gf = make_stride_windows(df, f.stem, window_size, stride, require_constant_label)\n",
    "        if Xf.size == 0:  #skip too-short or all-mixed windows\n",
    "            continue\n",
    "        all_X.append(Xf)\n",
    "        all_y.append(yf)\n",
    "        groups.extend(gf)\n",
    "\n",
    "    X = np.concatenate(all_X, axis=0) if all_X else np.empty((0, window_size, len(FEATURE_COLS)), dtype=np.float32)\n",
    "    y = np.concatenate(all_y, axis=0) if all_y else np.array([])\n",
    "    groups = np.array(groups)\n",
    "    return X, y, groups\n",
    "\n",
    "\n",
    "def build_cnn_with_fixed_norm_and_calibration(input_shape, num_classes, mean, std):\n",
    "    \n",
    "    #input_shape (T, C) like (120, 6)\n",
    "    \n",
    "    \n",
    "    C = input_shape[-1]\n",
    "    mean = np.asarray(mean, dtype=\"float32\").reshape((C,))\n",
    "    std  = np.asarray(std,  dtype=\"float32\").reshape((C,))\n",
    "    std  = np.maximum(std, 1e-8)\n",
    "    var  = (std ** 2).astype(\"float32\")\n",
    "\n",
    "    #fixed normalization\n",
    "    norm = layers.Normalization(axis=-1, name=\"fixed_normalization\")\n",
    "    norm.build((None,) + tuple(input_shape))  \n",
    "\n",
    "    \n",
    "    current_weights = norm.get_weights()\n",
    "    if len(current_weights) == 2:\n",
    "        \n",
    "        mean_t = mean.astype(\"float32\").reshape(current_weights[0].shape)\n",
    "        var_t  = var.astype(\"float32\").reshape(current_weights[1].shape)\n",
    "        norm.set_weights([mean_t, var_t])\n",
    "    elif len(current_weights) == 3:\n",
    "       \n",
    "        mean_t = mean.astype(\"float32\").reshape(current_weights[0].shape)\n",
    "        var_t  = var.astype(\"float32\").reshape(current_weights[1].shape)\n",
    "       \n",
    "        if current_weights[2].shape == ():\n",
    "            count_t = np.float32(1.0)  #scalar\n",
    "        else:\n",
    "            count_t = np.ones(current_weights[2].shape, dtype=\"float32\")\n",
    "        norm.set_weights([mean_t, var_t, count_t])\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            f\"Unexpected number of Normalization weights: {len(current_weights)}; \"\n",
    "            \"expected 2 or 3.\"\n",
    "        )\n",
    "    norm.trainable = False\n",
    "\n",
    "    #learnable per-channel calibration / light mixing\n",
    "    calib = layers.Conv1D(\n",
    "        filters=C, kernel_size=1, padding='same',\n",
    "        use_bias=True, name=\"calibration_conv1x1\"\n",
    "    )\n",
    "\n",
    "    model = models.Sequential([\n",
    "        layers.Input(shape=input_shape),\n",
    "        norm,\n",
    "        calib,\n",
    "\n",
    "        layers.Conv1D(64, 5, padding='same', activation=None),\n",
    "        layers.BatchNormalization(), layers.Activation('relu'),\n",
    "\n",
    "        layers.Conv1D(64, 5, padding='same', activation=None),\n",
    "        layers.BatchNormalization(), layers.Activation('relu'),\n",
    "        layers.MaxPooling1D(2),\n",
    "\n",
    "        layers.Conv1D(128, 5, padding='same', activation=None),\n",
    "        layers.BatchNormalization(), layers.Activation('relu'),\n",
    "\n",
    "        layers.GlobalAveragePooling1D(),\n",
    "        layers.Dense(128, activation='relu'),\n",
    "        layers.Dropout(0.3),\n",
    "        layers.Dense(num_classes, activation='softmax'),\n",
    "    ])\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='sparse_categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "X_raw, y_str, groups = load_and_window_all(\n",
    "    DATA_DIR, WINDOW_SIZE, STRIDE, require_constant_label=REQUIRE_CONSTANT_LABEL\n",
    ")\n",
    "print(f\"Raw windows: X={X_raw.shape}, y={y_str.shape}, unique labels={np.unique(y_str)}\")\n",
    "\n",
    "#Encode labels\n",
    "encoder = LabelEncoder()\n",
    "y = encoder.fit_transform(y_str)\n",
    "num_classes = len(encoder.classes_)\n",
    "print(\"Label mapping:\", dict(zip(encoder.classes_, range(num_classes))))\n",
    "\n",
    "\n",
    "logo = LeaveOneGroupOut()\n",
    "accuracies, histories = [], []\n",
    "fold = 1\n",
    "\n",
    "def fit_norm_from_train(X_train):\n",
    "  \n",
    "    mean = X_train.reshape(-1, X_train.shape[-1]).mean(axis=0)\n",
    "    std = X_train.reshape(-1, X_train.shape[-1]).std(axis=0)\n",
    "    std = np.where(std < 1e-8, 1.0, std)\n",
    "    return mean.astype(np.float32), std.astype(np.float32)\n",
    "\n",
    "for train_idx, test_idx in tqdm(list(logo.split(X_raw, y, groups)), desc=\"Cross-validation folds\"):\n",
    "    X_train_raw, X_test_raw = X_raw[train_idx], X_raw[test_idx]\n",
    "    y_train, y_test = y[train_idx], y[test_idx]\n",
    "\n",
    "    if USE_NORMALIZATION:\n",
    "        mean, std = fit_norm_from_train(X_train_raw)\n",
    "    else:\n",
    "        \n",
    "        mean = np.zeros(X_train_raw.shape[-1], dtype=np.float32)\n",
    "        std  = np.ones (X_train_raw.shape[-1], dtype=np.float32)\n",
    "\n",
    "   \n",
    "    model = build_cnn_with_fixed_norm_and_calibration(\n",
    "        input_shape=X_train_raw.shape[1:],\n",
    "        num_classes=num_classes,\n",
    "        mean=mean,\n",
    "        std=std\n",
    "    )\n",
    "\n",
    "    \n",
    "    ckpt_path = f\"{OUTPUT_DIR}/fold_{fold}_best.keras\"\n",
    "    cbs = [\n",
    "        callbacks.ModelCheckpoint(ckpt_path, monitor='val_accuracy', save_best_only=True, verbose=0),\n",
    "        callbacks.EarlyStopping(monitor='val_loss', patience=6, restore_best_weights=True)\n",
    "    ]\n",
    "\n",
    "    hist = model.fit(\n",
    "        X_train_raw, y_train,\n",
    "        validation_split=0.2,\n",
    "        epochs=EPOCHS,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        verbose=0,\n",
    "        callbacks=cbs\n",
    "    )\n",
    "    histories.append(hist)\n",
    "\n",
    "    loss, acc = model.evaluate(X_test_raw, y_test, verbose=0)\n",
    "    accuracies.append(acc)\n",
    "    print(f\"❖ Fold {fold}: Test Accuracy = {acc:.3f}\")\n",
    "    fold += 1\n",
    "\n",
    "print(f\"\\n Mean Accuracy = {np.mean(accuracies):.3f} ️ {np.std(accuracies):.3f}\")\n",
    "\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.bar(range(1, len(accuracies)+1), accuracies, color='skyblue')\n",
    "plt.title(\"Per-Fold Accuracy (Group CV by File)\")\n",
    "plt.xlabel(\"Fold\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.ylim(0, 1)\n",
    "for i, acc in enumerate(accuracies):\n",
    "    plt.text(i+1, min(acc+0.02, 0.98), f\"{acc:.2f}\", ha='center')\n",
    "plt.savefig(f\"{OUTPUT_DIR}/fold_accuracy.png\", dpi=300)\n",
    "plt.show()\n",
    "\n",
    "last_hist = histories[-1].history\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(last_hist['accuracy'], label='Train Acc', linewidth=2)\n",
    "plt.plot(last_hist['val_accuracy'], label='Val Acc', linewidth=2)\n",
    "plt.plot(last_hist['loss'], label='Train Loss', linestyle='--')\n",
    "plt.plot(last_hist['val_loss'], label='Val Loss', linestyle='--')\n",
    "plt.title(\"Training Progress (Last Fold)\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Value\")\n",
    "plt.legend()\n",
    "plt.savefig(f\"{OUTPUT_DIR}/training_curves_last_fold.png\", dpi=300)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "if USE_NORMALIZATION:\n",
    "    final_mean = X_raw.reshape(-1, X_raw.shape[-1]).mean(axis=0).astype(np.float32)\n",
    "    final_std  = X_raw.reshape(-1, X_raw.shape[-1]).std(axis=0).astype(np.float32)\n",
    "    final_std  = np.where(final_std < 1e-8, 1.0, final_std).astype(np.float32)\n",
    "else:\n",
    "    final_mean = np.zeros(X_raw.shape[-1], dtype=np.float32)\n",
    "    final_std  = np.ones (X_raw.shape[-1], dtype=np.float32)\n",
    "\n",
    "final_model = build_cnn_with_fixed_norm_and_calibration(\n",
    "    input_shape=(WINDOW_SIZE, len(FEATURE_COLS)),\n",
    "    num_classes=num_classes,\n",
    "    mean=final_mean,\n",
    "    std=final_std\n",
    ")\n",
    "\n",
    "cb_final = [\n",
    "    callbacks.ModelCheckpoint(f\"{OUTPUT_DIR}/final_best.keras\", monitor='val_accuracy', save_best_only=True),\n",
    "    callbacks.EarlyStopping(monitor='val_loss', patience=6, restore_best_weights=True)\n",
    "]\n",
    "\n",
    "\n",
    "final_history = final_model.fit(\n",
    "    X_raw, y,\n",
    "    validation_split=0.2,\n",
    "    epochs=EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    verbose=1,\n",
    "    callbacks=cb_final\n",
    ")\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(final_history.history['accuracy'], label='Train Acc', linewidth=2)\n",
    "plt.plot(final_history.history['val_accuracy'], label='Val Acc', linewidth=2)\n",
    "plt.plot(final_history.history['loss'], label='Train Loss', linestyle='--')\n",
    "plt.plot(final_history.history['val_loss'], label='Val Loss', linestyle='--')\n",
    "plt.title(\"Final Model Training\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Value\")\n",
    "plt.legend()\n",
    "plt.savefig(f\"{OUTPUT_DIR}/final_training_curve.png\", dpi=300)\n",
    "plt.show()\n",
    "\n",
    "y_pred = np.argmax(final_model.predict(X_raw, verbose=0), axis=1)\n",
    "cm = confusion_matrix(y, y_pred)\n",
    "plt.figure(figsize=(6,5))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=encoder.classes_, yticklabels=encoder.classes_)\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.title(\"Confusion Matrix (Final Model)\")\n",
    "plt.savefig(f\"{OUTPUT_DIR}/confusion_matrix.png\", dpi=300)\n",
    "plt.show()\n",
    "\n",
    "print(classification_report(y, y_pred, target_names=encoder.classes_))\n",
    "\n",
    "\n",
    "with open(f\"{OUTPUT_DIR}/metadata.json\", \"w\") as f:\n",
    "    json.dump({\n",
    "        \"label_classes\": encoder.classes_.tolist(),\n",
    "        \"feature_order\": FEATURE_COLS,\n",
    "        \"window_size\": WINDOW_SIZE,\n",
    "        \"stride_for_training\": STRIDE,\n",
    "        \"normalization_used\": USE_NORMALIZATION,\n",
    "        \"mean\": final_mean.tolist(),\n",
    "        \"std\":  final_std.tolist()\n",
    "    }, f, indent=2)\n",
    "\n",
    "tflite_path = f\"{OUTPUT_DIR}/imu_cnn_model.tflite\"\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(final_model)\n",
    "tflite_model = converter.convert()\n",
    "with open(tflite_path, \"wb\") as f:\n",
    "    f.write(tflite_model)\n",
    "\n",
    "print(f\"Exported TFLite model to {tflite_path}\")\n",
    "print(f\"Metadata saved to {OUTPUT_DIR}/metadata.json\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
